"""
Main Processing Pipeline for Video Enhancement SaaS

Orchestrates the complete end-to-end workflow:
1. Audio extraction and enhancement
2. Emphasis detection and entity recognition  
3. Image search and processing
4. Content matching and timing optimization
5. Video overlay generation
"""

import logging
import asyncio
import time
import os
from typing import Dict, List, Optional, Any, Tuple
from dataclasses import dataclass, field
from datetime import datetime
from pathlib import Path

# Import our services
from .audio.processor import AudioProcessor
from .transcription.whisper_service import WhisperService
from .emphasis.detector import MultiModalEmphasisDetector
from .nlp.entity_recognizer import EntityRecognizer
from .nlp.entity_enricher import EntityEnricher
from .images.image_searcher import ImageSearcher
from .images.image_processor import ImageProcessor
from .images.content_matcher import ContentMatcher, VideoSegment

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

@dataclass
class ProcessingConfig:
    """Configuration for processing pipeline."""
    # Audio processing
    enhance_audio: bool = True
    whisper_model: str = "base"
    
    # Emphasis detection
    emphasis_threshold: float = 0.3
    min_emphasis_duration: float = 1.0
    
    # Entity recognition
    entity_confidence_threshold: float = 0.5  # FIXED: Lower threshold for more entities
    max_entities_per_segment: int = 8         # FIXED: Allow more entities per segment
    
    # Image search
    max_images_per_entity: int = 2            # FIXED: Slightly reduced to balance
    image_quality_threshold: float = 0.5      # FIXED: Lower threshold for more images
    
    # Content matching
    max_overlays_per_video: int = 15          # FIXED: Allow many more overlays
    min_overlay_duration: float = 2.0
    max_overlay_duration: float = 6.0
    
    # Output format
    video_format: str = "portrait"  # portrait, landscape, square
    overlay_position: str = "top-right"
    overlay_opacity: float = 0.9

@dataclass
class ProcessingResult:
    """Result of complete processing pipeline."""
    # Input info
    video_path: str
    audio_path: str
    processing_config: ProcessingConfig
    
    # Processing results
    transcription: str
    emphasized_segments: List[Dict]
    recognized_entities: List[Any]
    enriched_entities: List[Any]
    image_results: List[Any]
    processed_images: List[Any]
    content_matches: List[Any]
    
    # Performance metrics
    processing_time: float
    audio_enhancement_time: float
    transcription_time: float
    emphasis_detection_time: float
    entity_recognition_time: float
    image_search_time: float
    image_processing_time: float
    content_matching_time: float
    video_creation_time: float
    
    # Quality scores
    overall_confidence: float
    emphasis_accuracy: float
    entity_recognition_accuracy: float
    image_match_quality: float
    
    # Output paths
    enhanced_audio_path: str = ""
    enhanced_video_path: str = ""
    overlay_config_path: str = ""
    processed_at: datetime = field(default_factory=datetime.now)

class ProcessingPipeline:
    """Complete video enhancement processing pipeline."""
    
    def __init__(self, config: Optional[ProcessingConfig] = None, job_id: Optional[str] = None, db_session = None):
        """Initialize the processing pipeline."""
        
        self.config = config or ProcessingConfig()
        self.job_id = job_id
        self.db_session = db_session
        
        # Initialize all services
        self.audio_processor = AudioProcessor()
        self.whisper_service = WhisperService()
        self.emphasis_detector = MultiModalEmphasisDetector()
        self.entity_recognizer = EntityRecognizer()
        self.entity_enricher = EntityEnricher()
        self.image_searcher = ImageSearcher()
        self.image_processor = ImageProcessor()
        self.content_matcher = ContentMatcher()
        
        # Performance tracking
        self.processing_stats = {
            'total_videos_processed': 0,
            'total_processing_time': 0.0,
            'average_processing_time': 0.0,
            'success_rate': 0.0
        }
        
    async def process_video(self, video_path: str, output_dir: str = "./output") -> ProcessingResult:
        """
        Process a complete video through the enhancement pipeline.
        
        Args:
            video_path: Path to input video file
            output_dir: Directory for output files
            
        Returns:
            ProcessingResult with all pipeline outputs
        """
        
        try:
            start_time = time.time()
            logger.info(f"Starting video processing pipeline for: {video_path}")
            
            # Initialize result object
            result = ProcessingResult(
                video_path=video_path,
                audio_path="",
                processing_config=self.config,
                transcription="",
                emphasized_segments=[],
                recognized_entities=[],
                enriched_entities=[],
                image_results=[],
                processed_images=[],
                content_matches=[],
                processing_time=0.0,
                audio_enhancement_time=0.0,
                transcription_time=0.0,
                emphasis_detection_time=0.0,
                entity_recognition_time=0.0,
                image_search_time=0.0,
                image_processing_time=0.0,
                content_matching_time=0.0,
                video_creation_time=0.0,
                overall_confidence=0.0,
                emphasis_accuracy=0.0,
                entity_recognition_accuracy=0.0,
                image_match_quality=0.0
            )
            
            # Ensure output directory exists
            output_path = Path(output_dir)
            output_path.mkdir(parents=True, exist_ok=True)
            
            # Step 1: Extract and enhance audio
            step_start = time.time()
            logger.info("Step 1: Audio extraction and enhancement")
            
            audio_analysis_path = await self._extract_and_enhance_audio(video_path, output_path)
            result.audio_path = audio_analysis_path  # This should be the video path
            result.enhanced_audio_path = audio_analysis_path  # This should also be the video path  
            result.audio_enhancement_time = time.time() - step_start
            
            # CRITICAL: Ensure we're working with video files, not audio files
            if audio_analysis_path.endswith('.wav') or audio_analysis_path.endswith('.mp3'):
                logger.error(f"CRITICAL BUG: Audio path returned instead of video: {audio_analysis_path}")
                result.audio_path = video_path
                result.enhanced_audio_path = video_path
            
            # Step 2: Transcribe audio with Whisper
            step_start = time.time()
            logger.info("Step 2: Audio transcription")
            
            transcription_result = await self._transcribe_audio(audio_analysis_path)
            result.transcription = transcription_result.get('text', '')
            result.transcription_time = time.time() - step_start
            
            # Step 3: Detect emphasized segments
            step_start = time.time()
            logger.info("Step 3: Emphasis detection")
            
            emphasized_segments = await self._detect_emphasis(audio_analysis_path, transcription_result)
            result.emphasized_segments = emphasized_segments
            result.emphasis_detection_time = time.time() - step_start
            
            # Step 4: Extract and enrich entities
            step_start = time.time()
            logger.info("Step 4: Entity recognition and enrichment")
            
            entities, enriched_entities = await self._extract_and_enrich_entities(
                result.transcription, emphasized_segments
            )
            result.recognized_entities = entities
            result.enriched_entities = enriched_entities
            result.entity_recognition_time = time.time() - step_start
            
            # Step 5: Search for relevant images
            step_start = time.time() 
            logger.info("Step 5: Image search")
            
            try:
                image_results = await self._search_images(enriched_entities)
                result.image_results = image_results
                result.image_search_time = time.time() - step_start
            except Exception as e:
                import traceback
                logger.error(f"Detailed error in Step 5: {e}")
                logger.error(f"Full traceback: {traceback.format_exc()}")
                raise
            
            # Step 6: Process images for overlay
            step_start = time.time()
            logger.info("Step 6: Image processing")
            
            processed_images = await self._process_images(image_results)
            result.processed_images = processed_images
            result.image_processing_time = time.time() - step_start
            
            # Step 7: Match content and optimize timing
            step_start = time.time()
            logger.info("Step 7: Content matching and timing optimization")
            
            content_matches = await self._match_content(
                emphasized_segments, enriched_entities, processed_images
            )
            result.content_matches = content_matches
            result.content_matching_time = time.time() - step_start
            
            # Step 8: Create final enhanced video
            step_start = time.time()
            logger.info("Step 8: Creating final enhanced video")
            
            enhanced_video_path = await self._create_enhanced_video(
                video_path, result.transcription, emphasized_segments, 
                enriched_entities, processed_images, content_matches, output_path
            )
            
            # CRITICAL DEBUG: Log what _create_enhanced_video returned
            logger.info(f"_create_enhanced_video returned: {enhanced_video_path}")
            
            # EMERGENCY FIX: If enhanced_video_path is somehow an audio file, force create a video file
            if enhanced_video_path and (enhanced_video_path.endswith('.wav') or enhanced_video_path.endswith('.mp3') or enhanced_video_path.endswith('.aac')):
                logger.error(f"CRITICAL BUG: _create_enhanced_video returned audio file: {enhanced_video_path}")
                import shutil
                video_filename = f"emergency_video_{int(time.time())}.mp4"
                emergency_video_path = output_path / video_filename
                if Path(video_path).exists():
                    logger.info(f"EMERGENCY REPAIR: Copying original video {video_path} -> {emergency_video_path}")
                    await asyncio.get_event_loop().run_in_executor(
                        None, shutil.copy2, video_path, emergency_video_path
                    )
                    enhanced_video_path = str(emergency_video_path)
                else:
                    logger.error(f"EMERGENCY FALLBACK: Original video not found: {video_path}")
                    enhanced_video_path = video_path
            
            # CRITICAL FIX: Ensure we always have a video file, never audio
            if enhanced_video_path and (enhanced_video_path.endswith('.wav') or enhanced_video_path.endswith('.mp3') or enhanced_video_path.endswith('.aac')):
                logger.error(f"ERROR: Enhanced video path is audio file: {enhanced_video_path}")
                # Force copy original video as fallback
                import shutil
                video_filename = f"enhanced_video_{int(time.time())}.mp4"
                video_path_fixed = output_path / video_filename
                if Path(video_path).exists():
                    logger.info(f"EMERGENCY FIX: Copying original video {video_path} -> {video_path_fixed}")
                    await asyncio.get_event_loop().run_in_executor(
                        None, shutil.copy2, video_path, video_path_fixed
                    )
                    enhanced_video_path = str(video_path_fixed)
                else:
                    logger.error(f"FALLBACK ERROR: Original video not found: {video_path}")
                    enhanced_video_path = video_path  # Use original path as last resort
            
            # FINAL VALIDATION: Ensure we have a video extension
            if enhanced_video_path and not any(enhanced_video_path.lower().endswith(ext) for ext in ['.mp4', '.mov', '.avi', '.webm', '.mkv']):
                logger.error(f"ERROR: Final path doesn't have video extension: {enhanced_video_path}")
                # Force .mp4 extension if missing
                if not enhanced_video_path.endswith('.'):
                    enhanced_video_path += '.mp4'
                else:
                    enhanced_video_path += 'mp4'
                logger.info(f"FIXED: Added video extension: {enhanced_video_path}")
            
            # FINAL SAFETY CHECK: Absolutely ensure we never set a .wav file as enhanced_video_path
            if enhanced_video_path and any(enhanced_video_path.lower().endswith(ext) for ext in ['.wav', '.mp3', '.aac', '.flac', '.ogg']):
                logger.error(f"FINAL SAFETY VIOLATION: About to set audio file as enhanced_video_path: {enhanced_video_path}")
                logger.error("This should NEVER happen! Forcing video path...")
                enhanced_video_path = video_path  # Use original video as last resort
            
            result.enhanced_video_path = enhanced_video_path
            result.video_creation_time = time.time() - step_start
            
            logger.info(f"FINAL enhanced_video_path: {result.enhanced_video_path}")
            
            # EXTRA VALIDATION: Double check the result object
            if result.enhanced_video_path and any(result.enhanced_video_path.lower().endswith(ext) for ext in ['.wav', '.mp3', '.aac']):
                logger.error(f"VALIDATION FAILED: Result object contains audio path: {result.enhanced_video_path}")
                result.enhanced_video_path = video_path
            
            # Calculate final metrics
            result = self._calculate_quality_metrics(result)
            
            # CRITICAL FIX: Set the total processing time
            result.processing_time = time.time() - start_time
            
            # Save results
            await self._save_results(result, output_path)
            
            # Update stats
            self._update_processing_stats(result)
            
            logger.info(f"Pipeline completed in {result.processing_time:.2f}s with {result.overall_confidence:.1%} confidence")
            return result
            
        except Exception as e:
            logger.error(f"Error in processing pipeline: {e}")
            raise
    
    async def _extract_and_enhance_audio(self, video_path: str, output_path: Path) -> str:
        """Extract and enhance audio from video."""
        
        # CRITICAL FIX: Always return the video path to avoid audio/video confusion
        # The audio processing is for transcription/analysis only, not for final output
        logger.info(f"Audio processing step: analyzing video file {video_path}")
        
        if self.config.enhance_audio:
            # For transcription purposes, we can extract audio temporarily
            # but we NEVER return the audio path as the final result
            try:
                # Extract audio for processing (temporary)
                processed_audio = await self.audio_processor.extract_and_process(video_path)
                logger.info(f"Audio extracted for analysis: {processed_audio.duration:.2f}s")
                # Audio file is temporary and will be cleaned up automatically
            except Exception as e:
                logger.warning(f"Audio processing failed: {e}, continuing with video file")
        
        # ALWAYS return the original video path, never an audio path
        # This ensures enhanced_video_path is always a video file
        return video_path
    
    async def _transcribe_audio(self, audio_path: str) -> Dict:
        """Transcribe audio using Whisper."""
        
        try:
            logger.info(f"ðŸŽ¤ REAL TRANSCRIPTION: Processing audio from {audio_path}")
            
            # CRITICAL FIX: Extract audio from video for transcription
            if audio_path.endswith('.MOV') or audio_path.endswith('.mp4') or audio_path.endswith('.avi'):
                logger.info(f"ðŸŽ¤ Extracting audio from video file: {audio_path}")
                # Extract audio temporarily for transcription
                import tempfile
                temp_audio_path = tempfile.mktemp(suffix='.wav')
                
                # Use FFmpeg to extract audio
                import ffmpeg
                await asyncio.get_event_loop().run_in_executor(
                    None,
                    lambda: (
                        ffmpeg
                        .input(audio_path)
                        .output(temp_audio_path, acodec='pcm_s16le', ar=16000, ac=1)
                        .overwrite_output()
                        .run(quiet=True)
                    )
                )
                audio_file_path = temp_audio_path
                logger.info(f"ðŸŽ¤ Audio extracted to: {audio_file_path}")
            else:
                audio_file_path = audio_path
            
            # Check if audio file exists
            if not os.path.exists(audio_file_path):
                logger.error(f"ðŸŽ¤ Audio file not found: {audio_file_path}")
                raise FileNotFoundError(f"Audio file not found: {audio_file_path}")
            
            logger.info(f"ðŸŽ¤ Using audio file: {audio_file_path}")
            
            # Create AudioSegment object for Whisper service
            logger.info(f"ðŸŽ¤ Creating AudioSegment for Whisper service...")
            
            # Get audio duration for the segment
            try:
                import librosa
                y, sr = librosa.load(audio_file_path, sr=None)
                duration = len(y) / sr
                logger.info(f"ðŸŽ¤ Audio duration: {duration:.2f} seconds")
            except Exception as e:
                logger.warning(f"ðŸŽ¤ Could not get audio duration: {e}, using default")
                duration = 60  # Default duration
            
            # SIMPLIFIED APPROACH: Use basic Whisper directly
            logger.info(f"ðŸŽ¤ Using direct Whisper transcription...")
            
            try:
                import whisper
                # Load a simple Whisper model directly  
                model = whisper.load_model("base")
                result = model.transcribe(audio_file_path)
                
                if result and 'text' in result:
                    # Convert to our expected format
                    segments = []
                    if 'segments' in result and isinstance(result['segments'], list):
                        for seg in result['segments']:
                            if isinstance(seg, dict):
                                segments.append({
                                    'start': seg.get('start', 0.0) if hasattr(seg, 'get') else 0.0,
                                    'end': seg.get('end', 1.0) if hasattr(seg, 'get') else 1.0,
                                    'text': str(seg.get('text', '') if hasattr(seg, 'get') else '').strip()
                                })
                    
                    text_result = result['text']
                    if isinstance(text_result, list):
                        text_result = ' '.join(str(x) for x in text_result)
                    
                    real_transcription = {
                        'text': str(text_result).strip(),
                        'segments': segments,
                        'language': str(result.get('language', 'en') if hasattr(result, 'get') else 'en')
                    }
                    
                    logger.info(f"ðŸŽ¤ DIRECT WHISPER SUCCESS: '{real_transcription['text'][:100]}...'")
                    logger.info(f"ðŸŽ¤ Found {len(real_transcription['segments'])} segments")
                    
                    # Clean up temporary audio file
                    if audio_file_path != audio_path and os.path.exists(audio_file_path):
                        os.remove(audio_file_path)
                    
                    return real_transcription
                else:
                    logger.warning(f"ðŸŽ¤ Direct Whisper returned no text")
                    
            except Exception as whisper_error:
                logger.error(f"ðŸŽ¤ Direct Whisper failed: {whisper_error}")
                # Continue to fallback
                
        except Exception as e:
            logger.error(f"ðŸŽ¤ REAL TRANSCRIPTION FAILED: {e}")
            logger.warning(f"ðŸŽ¤ Falling back to basic extraction...")
            
        # Fallback: Basic transcription attempt
        try:
            # Simple fallback - return empty but properly structured result
            fallback_transcription = {
                'text': f"[Audio transcription from {os.path.basename(audio_path)}]",
                'segments': [
                    {'start': 0.0, 'end': 10.0, 'text': '[Transcription in progress...]'}
                ],
                'language': 'en'
            }
            
            logger.warning(f"ðŸŽ¤ Using fallback transcription structure")
            return fallback_transcription
            
        except Exception as e:
            logger.error(f"ðŸŽ¤ Even fallback transcription failed: {e}")
            # Return minimal structure to prevent crashes
            return {
                'text': '[Transcription unavailable]',
                'segments': [],
                'language': 'en'
            }
    
    async def _detect_emphasis(self, audio_path: str, transcription: Dict) -> List[Dict]:
        """Detect emphasized segments in audio."""
        
        try:
            logger.info(f"ðŸŽ¯ EMPHASIS DETECTION: Processing {audio_path}")
            
            # Get transcription data
            segments = transcription.get('segments', [])
            transcript_text = transcription.get('text', '')
            
            if not transcript_text or not segments:
                logger.warning("ðŸŽ¯ No transcription data available for emphasis detection")
                return []
            
            # Create word timestamps from segments
            word_timestamps = []
            for segment in segments:
                segment_text = segment.get('text', '').strip()
                segment_start = segment.get('start', 0.0)
                segment_end = segment.get('end', segment_start + 1.0)
                segment_duration = segment_end - segment_start
                
                words = segment_text.split()
                if not words:
                    continue
                    
                word_duration = segment_duration / len(words)
                
                for i, word in enumerate(words):
                    clean_word = word.strip('.,!?:;"()[]{}')
                    if not clean_word:
                        continue
                        
                    word_start = segment_start + (i * word_duration)
                    word_end = word_start + word_duration
                    
                    word_timestamps.append({
                        'word': clean_word,
                        'start': word_start,
                        'end': word_end
                    })
            
            if not word_timestamps:
                logger.warning("ðŸŽ¯ No word timestamps available")
                return []
                
            logger.info(f"ðŸŽ¯ Created {len(word_timestamps)} word timestamps")
            
            # Load audio data for the emphasis detector
            try:
                import librosa
                import numpy as np
                
                # Extract audio if it's a video file
                if audio_path.endswith(('.MOV', '.mp4', '.m4v', '.avi')):
                    import tempfile
                    import ffmpeg
                    
                    temp_audio = tempfile.mktemp(suffix='.wav')
                    try:
                        (
                            ffmpeg
                            .input(audio_path)
                            .output(temp_audio, acodec='pcm_s16le', ar=16000, ac=1)
                            .overwrite_output()
                            .run(quiet=True, capture_stdout=True)
                        )
                        audio_data, sr = librosa.load(temp_audio, sr=16000)
                        os.remove(temp_audio)
                    except Exception as e:
                        logger.error(f"ðŸŽ¯ Audio extraction failed: {e}")
                        return []
                else:
                    audio_data, sr = librosa.load(audio_path, sr=16000)
                
                # Format for the sophisticated MultiModalEmphasisDetector
                transcription_result = {
                    'word_timestamps': word_timestamps,
                    'text': transcript_text
                }
                
                logger.info(f"ðŸŽ¯ Calling MultiModalEmphasisDetector...")
                
                # Use the existing sophisticated emphasis detection system
                emphasis_results = self.emphasis_detector.detect_emphasis(
                    audio_data=audio_data,
                    transcription_result=transcription_result
                )
                
                logger.info(f"ðŸŽ¯ Emphasis detector returned {len(emphasis_results)} results")
                
                # Convert EmphasisResult objects to expected pipeline format
                formatted_segments = []
                for result in emphasis_results:
                    formatted_segment = {
                        'start_time': result.start,
                        'end_time': result.end,
                        'confidence': result.confidence,
                        'text': result.word,
                        'word': result.word,
                        'emphasis_score': result.emphasis_score,
                        'is_emphasized': result.is_emphasized,
                        'emphasis_type': 'strong' if result.emphasis_score > 0.7 else ('moderate' if result.emphasis_score > 0.4 else 'weak'),
                        'audio_features': result.analysis_details
                    }
                    formatted_segments.append(formatted_segment)
                
                emphasized_count = sum(1 for seg in formatted_segments if seg['is_emphasized'])
                logger.info(f"ðŸŽ¯ EMPHASIS SUCCESS: Found {emphasized_count} emphasized words out of {len(formatted_segments)} total")
                
                return formatted_segments
                
            except Exception as e:
                logger.error(f"ðŸŽ¯ Emphasis detection failed: {e}")
                import traceback
                logger.error(f"ðŸŽ¯ Traceback: {traceback.format_exc()}")
                return []
            
        except Exception as e:
            logger.error(f"ðŸŽ¯ Emphasis detection error: {e}")
            return []
    
    async def _extract_and_enrich_entities(self, transcription: str, emphasized_segments: List[Dict]) -> Tuple[List[Any], List[Any]]:
        """Extract and enrich entities from transcription."""
        
        try:
            logger.info(f"ðŸ·ï¸  ENTITY EXTRACTION: Starting for transcript: '{transcription[:100]}...'")
            
            if not transcription or transcription.strip() == "":
                logger.warning("ðŸ·ï¸  Empty transcription provided, cannot extract entities")
                return [], []
            
            # Try entity recognition with better error handling
            try:
                logger.info(f"ðŸ·ï¸  Calling entity recognizer...")
                entities = await asyncio.get_event_loop().run_in_executor(
                    None, self.entity_recognizer.recognize_entities, transcription
                )
                logger.info(f"ðŸ·ï¸  ENTITY EXTRACTION SUCCESS: Found {len(entities)} entities")
                
                for i, entity in enumerate(entities[:5]):  # Log first 5 entities
                    logger.info(f"ðŸ·ï¸    Entity {i+1}: '{entity.text}' ({entity.entity_type.value}) - confidence: {entity.confidence:.2f}")
                    
            except Exception as entity_error:
                logger.error(f"ðŸ·ï¸  Entity recognition failed: {entity_error}")
                # Try a simplified fallback approach
                logger.info(f"ðŸ·ï¸  Attempting fallback entity extraction...")
                
                try:
                    # Simple fallback: extract basic entities from transcript using basic regex
                    import re
                    
                    # Look for capitalized words that might be entities
                    words = transcription.split()
                    potential_entities = []
                    
                    for word in words:
                        # Clean the word
                        clean_word = re.sub(r'[^\w]', '', word)
                        
                        # Check if it's a potential entity (capitalized, > 2 chars, not common words)
                        if (len(clean_word) > 2 and 
                            clean_word[0].isupper() and 
                            clean_word.lower() not in ['the', 'and', 'but', 'this', 'that', 'with', 'for', 'are', 'you', 'they', 'have', 'been', 'said', 'will', 'can', 'get']):
                            
                            # Create a basic entity object (simplified)
                            try:
                                from .nlp.entity_recognizer import EntityResult, EntityType, ImagePotential
                                entity = EntityResult(
                                    text=clean_word,
                                    entity_type=EntityType.MISC,  # Default type
                                    start_char=0,
                                    end_char=len(clean_word),
                                    confidence=0.7,  # Moderate confidence for fallback
                                    image_potential=ImagePotential.MODERATE,
                                    canonical_name=clean_word,
                                    aliases=[],
                                    category='FALLBACK',
                                    context_score=0.5,
                                    recognition_sources=['fallback'],
                                    spacy_label=None,
                                    regex_pattern=None
                                )
                                potential_entities.append(entity)
                            except ImportError:
                                # If we can't import the classes, create a simple dict
                                entity = {
                                    'text': clean_word,
                                    'entity_type': 'MISC',
                                    'confidence': 0.7,
                                    'canonical_name': clean_word
                                }
                                potential_entities.append(entity)
                    
                    entities = potential_entities[:5]  # Limit to 5 entities
                    logger.info(f"ðŸ·ï¸  FALLBACK ENTITIES: Found {len(entities)} potential entities")
                    for i, entity in enumerate(entities):
                        entity_text = entity.text if hasattr(entity, 'text') else entity.get('text', 'unknown')
                        logger.info(f"ðŸ·ï¸    Fallback {i+1}: '{entity_text}'")
                        
                except Exception as fallback_error:
                    logger.error(f"ðŸ·ï¸  Even fallback entity extraction failed: {fallback_error}")
                    entities = []
            
            # Try entity enrichment with better error handling  
            try:
                if entities:
                    logger.info(f"ðŸ·ï¸  Calling entity enricher for {len(entities)} entities...")
                    enriched_entities = await asyncio.get_event_loop().run_in_executor(
                        None, self.entity_enricher.enrich_entities, entities
                    )
                    logger.info(f"ðŸ·ï¸  ENTITY ENRICHMENT SUCCESS: Enriched {len(enriched_entities)} entities")
                    
                    for i, entity in enumerate(enriched_entities[:5]):  # Log first 5 enriched entities
                        entity_text = getattr(entity, 'text', getattr(entity, 'canonical_name', 'unknown'))
                        image_potential = getattr(entity, 'image_potential', 'unknown')
                        logger.info(f"ðŸ·ï¸    Enriched {i+1}: '{entity_text}' - image_potential: {image_potential}")
                else:
                    logger.warning(f"ðŸ·ï¸  No entities to enrich")
                    enriched_entities = []
                    
            except Exception as enrichment_error:
                logger.error(f"ðŸ·ï¸  Entity enrichment failed: {enrichment_error}")
                logger.info(f"ðŸ·ï¸  Using non-enriched entities")
                enriched_entities = entities  # Use original entities if enrichment fails
            
            # ðŸš¨ CRITICAL FIX: Store entities to database if we have job_id and db_session
            if self.job_id and self.db_session and enriched_entities:
                logger.info(f"ðŸ’¾ Storing {len(enriched_entities)} enriched entities to database for job {self.job_id}")
                try:
                    from app.database.models import EnrichedEntity
                    import uuid
                    from datetime import datetime
                    
                    for entity in enriched_entities:
                        # Create EnrichedEntity record
                        db_entity = EnrichedEntity(
                            id=str(uuid.uuid4()),
                            job_id=self.job_id,
                            text=getattr(entity, 'text', ''),
                            normalized_text=getattr(entity, 'canonical_name', getattr(entity, 'text', '')).lower(),
                            entity_type=getattr(entity, 'entity_type', 'UNKNOWN'),
                            confidence=float(getattr(entity, 'confidence', 0.0)),
                            start_char=getattr(entity, 'start_char', 0),
                            end_char=getattr(entity, 'end_char', 0),
                            description=f"Entity extracted from transcript: {getattr(entity, 'canonical_name', '')}",
                            search_queries=getattr(entity, 'search_queries', []),
                            emphasis_strength=getattr(entity, 'context_score', 0.0),
                            created_at=datetime.utcnow()
                        )
                        
                        self.db_session.add(db_entity)
                        logger.info(f"ðŸ’¾ Added EnrichedEntity to database: {db_entity.text}")
                    
                    # Commit the database changes
                    self.db_session.commit()
                    logger.info(f"âœ… Successfully stored {len(enriched_entities)} entities to database")
                    
                except Exception as e:
                    import traceback
                    logger.error(f"âŒ Failed to store entities to database: {e}")
                    logger.error(f"Full traceback: {traceback.format_exc()}")
                    # Don't fail the whole pipeline if database storage fails
                    pass
            
            logger.info(f"ðŸ·ï¸  FINAL RESULT: {len(entities)} entities, {len(enriched_entities)} enriched")
            return entities, enriched_entities
            
        except Exception as e:
            import traceback
            logger.error(f"ðŸ·ï¸  CRITICAL ERROR in entity extraction: {e}")
            logger.error(f"ðŸ·ï¸  Full traceback: {traceback.format_exc()}")
            
            # DON'T return empty lists - instead, try one more fallback
            logger.warning(f"ðŸ·ï¸  Attempting emergency entity fallback...")
            
            try:
                # Emergency fallback: just look for common entity patterns
                import re
                emergency_entities = []
                
                # Look for country names, people names, companies
                patterns = {
                    'countries': r'\b(Iran|Israel|America|USA|China|Russia|France|Germany|Japan|India|Brazil|Mexico|Canada|Australia|Italy|Spain|Egypt|Turkey|Pakistan|Nigeria|Argentina)\b',
                    'people': r'\b(Biden|Trump|Putin|Musk|Gates|Bezos|Zuckerberg|Jobs|Obama|Clinton|Bush|Reagan|Roosevelt|Washington|Lincoln|Kennedy)\b', 
                    'companies': r'\b(Apple|Google|Microsoft|Amazon|Facebook|Tesla|Netflix|Twitter|Intel|Samsung|Sony|Toyota|McDonald|Coca-Cola|Disney|Nike)\b',
                }
                
                for category, pattern in patterns.items():
                    matches = re.findall(pattern, transcription, re.IGNORECASE)
                    for match in matches:
                        emergency_entities.append({
                            'text': match,
                            'canonical_name': match,
                            'entity_type': 'EMERGENCY_FALLBACK',
                            'confidence': 0.8,
                            'image_potential': 'EXCELLENT'
                        })
                
                logger.info(f"ðŸ·ï¸  EMERGENCY FALLBACK: Found {len(emergency_entities)} entities")
                for entity in emergency_entities:
                    logger.info(f"ðŸ·ï¸    Emergency: '{entity['text']}'")
                
                return emergency_entities, emergency_entities
                
            except Exception as emergency_error:
                logger.error(f"ðŸ·ï¸  Even emergency fallback failed: {emergency_error}")
                return [], []
    
    async def _search_images(self, enriched_entities: List[Any]) -> List[Any]:
        """Search for images for entities."""
        
        image_results = []
        
        for entity in enriched_entities[:self.config.max_entities_per_segment]:
            # Skip low-quality entities - image_potential is an enum ('excellent', 'good', 'poor')
            if hasattr(entity, 'image_potential'):
                image_potential_str = str(entity.image_potential).lower()
                # Only skip if explicitly marked as 'poor' or 'none'
                if image_potential_str in ['poor', 'none', 'low']:
                    continue

            # Search for images
            entity_images = await self.image_searcher.search_for_entity(entity)

            # Limit results per entity
            entity_images = entity_images[:self.config.max_images_per_entity]
            image_results.extend(entity_images)
        
        logger.info(f"Found {len(image_results)} images for {len(enriched_entities)} entities")
        return image_results
    
    async def _process_images(self, image_results: List[Any]) -> List[Any]:
        """Process images for video overlay."""
        
        processed_images = []
        
        async with self.image_processor as processor:
            # Get processing options based on video format
            preset_options = processor.get_preset_options(self.config.video_format)

            # Process images in parallel (batches of 5)
            batch_size = 5
            for i in range(0, len(image_results), batch_size):
                batch = image_results[i:i + batch_size]
                batch_urls = [getattr(img, 'url', '') for img in batch]

                if batch_urls:
                    batch_processed = await processor.process_multiple_images(
                        batch_urls, preset_options
                    )

                    # Preserve entity metadata by attaching attributes from the
                    # original ImageResult to each ProcessedImage
                    for original, processed in zip(batch, batch_processed):
                        if processed:
                            setattr(processed, 'entity_name', getattr(original, 'entity_name', 'unknown'))
                            setattr(processed, 'entity_type', getattr(original, 'entity_type', 'UNKNOWN'))
                            setattr(processed, 'search_query', getattr(original, 'search_query', ''))
                            setattr(processed, 'source', getattr(original, 'source', ''))
                            setattr(processed, 'url', getattr(original, 'url', ''))
                            # Provide stable identifier for later matching
                            setattr(processed, 'image_id', getattr(processed, 'cache_key', None))

                    processed_images.extend(batch_processed)
        
        # ðŸš¨ CRITICAL FIX: Store images to database if we have job_id and db_session
        if self.job_id and self.db_session and processed_images:
            logger.info(f"ðŸ’¾ Storing {len(processed_images)} processed images to database for job {self.job_id}")
            try:
                from app.database.models import StoredImage
                import uuid
                from datetime import datetime
                
                for img in processed_images:
                    # Create StoredImage record
                    stored_image = StoredImage(
                        id=str(uuid.uuid4()),
                        job_id=self.job_id,
                        original_url=getattr(img, 'original_url', '') or getattr(img, 'url', ''),
                        original_provider=getattr(img, 'source', 'unsplash'),
                        s3_key=getattr(img, 'processed_path', ''),
                        s3_bucket='video-enhancement-images',  # Default bucket
                        cdn_urls={'original': getattr(img, 'processed_path', '')},
                        entity_name=getattr(img, 'entity_name', 'unknown'),
                        entity_type=getattr(img, 'entity_type', 'UNKNOWN'),
                        original_width=getattr(img, 'width', 0),
                        original_height=getattr(img, 'height', 0),
                        original_file_size=getattr(img, 'file_size', 0),
                        quality_score=0.8,  # Default score
                        relevance_score=0.8,  # Default score
                        aesthetic_score=0.8,  # Default score
                        ranking_score=0.8,  # Default score
                        processing_status='completed',
                        created_at=datetime.utcnow()
                    )
                    
                    self.db_session.add(stored_image)
                    logger.info(f"ðŸ’¾ Added StoredImage to database: {stored_image.entity_name}")
                
                # Commit the database changes
                self.db_session.commit()
                logger.info(f"âœ… Successfully stored {len(processed_images)} images to database")
                
            except Exception as e:
                import traceback
                logger.error(f"âŒ Failed to store images to database: {e}")
                logger.error(f"Full traceback: {traceback.format_exc()}")
                # Don't fail the whole pipeline if database storage fails
                pass
        
        logger.info(f"Processed {len(processed_images)}/{len(image_results)} images")
        return processed_images
    
    async def _match_content(self, emphasized_segments: List[Dict], 
                           enriched_entities: List[Any], 
                           processed_images: List[Any]) -> List[Any]:
        """Match content and optimize timing."""
        
        # Create video segments from emphasis results
        video_segments = []
        for segment in emphasized_segments:
            # Find entities in this segment
            segment_entities = []
            for entity in enriched_entities:
                # Simple timing check (would be more sophisticated in production)
                if hasattr(entity, 'start_time'):
                    entity_time = entity.start_time
                else:
                    entity_time = segment['start_time']  # Default to segment start
                
                if segment['start_time'] <= entity_time <= segment['end_time']:
                    segment_entities.append(entity.canonical_name)
            
            video_segment = VideoSegment(
                start_time=segment['start_time'],
                end_time=segment['end_time'],
                emphasized_entities=segment_entities,
                confidence=segment['confidence'],
                text_content=segment.get('text', '')
            )
            video_segments.append(video_segment)
        
        # Match images to segments
        content_matches = await self.content_matcher.match_images_to_video(
            video_segments,
            processed_images,
            self.config.video_format,
            max_matches=self.config.max_overlays_per_video,
        )
        
        logger.info(f"Created {len(content_matches)} content matches")
        return content_matches
    
    async def _create_enhanced_video(self, video_path: str, transcription: str, 
                                   emphasized_segments: List[Dict], enriched_entities: List[Any],
                                   processed_images: List[Any], content_matches: List[Any], 
                                   output_path: Path) -> str:
        """Create final enhanced video with captions and overlays using VideoComposer."""
        
        try:
            # Always create a video file first as guaranteed fallback
            import shutil
            enhanced_video_filename = f"enhanced_video_{int(time.time())}.mp4"
            enhanced_video_path = output_path / enhanced_video_filename
            
            # Copy original video as base
            if Path(video_path).exists():
                logger.info(f"Creating base video file: {video_path} -> {enhanced_video_path}")
                await asyncio.get_event_loop().run_in_executor(
                    None, shutil.copy2, video_path, enhanced_video_path
                )
            else:
                logger.error(f"Original video not found: {video_path}")
                return video_path
            
            # DETAILED DEBUG: Log what we're passing to VideoComposer
            logger.info(f"ðŸŽ¬ VIDEOCOMPOSER DEBUG:")
            logger.info(f"ðŸŽ¬ Input video: {video_path}")
            logger.info(f"ðŸŽ¬ Transcription length: {len(transcription) if transcription else 0} characters")
            logger.info(f"ðŸŽ¬ Emphasis points: {len(emphasized_segments)} segments")
            logger.info(f"ðŸŽ¬ Enriched entities: {len(enriched_entities)} entities")
            logger.info(f"ðŸŽ¬ Processed images: {len(processed_images)} images") 
            logger.info(f"ðŸŽ¬ Content matches: {len(content_matches)} matches")
            
            # Log details of processed images
            for i, img in enumerate(processed_images[:3]):  # Show first 3
                logger.info(f"ðŸŽ¬ Image {i}: type={type(img)}, attrs={dir(img) if hasattr(img, '__dict__') else 'N/A'}")
                if hasattr(img, '__dict__'):
                    logger.info(f"ðŸŽ¬ Image {i} data: {img.__dict__}")
            
            # Now try to use VideoComposer to enhance it
            try:
                from app.services.composition.video_composer import VideoComposer
                from app.services.composition.models import CompositionConfig
                
                logger.info("ðŸŽ¬ Attempting VideoComposer enhancement...")
                
                # Initialize VideoComposer with platform-optimized settings
                config = CompositionConfig(
                    output_resolution=(1080, 1920) if self.config.video_format == "portrait" else (1920, 1080),
                    output_fps=30,
                    output_bitrate="5M",
                    preset="fast",
                    gpu_acceleration=True
                )
                
                video_composer = VideoComposer(config)
                
                # Prepare composition data for VideoComposer
                composition_data = {
                    'input_video_path': video_path,
                    'transcript': transcription,
                    'emphasis_points': emphasized_segments,
                    'entities': enriched_entities,
                    'curated_images': processed_images,
                    'content_matches': content_matches,
                    'selected_style': {
                        'template_name': 'dynamic_overlay',
                        'show_captions': True,
                        'image_transition_duration': 0.5,
                        'overlay_opacity': self.config.overlay_opacity
                    },
                    'audio_features': {
                        'enhancement_applied': self.config.enhance_audio
                    }
                }
                
                logger.info(f"ðŸŽ¬ Calling VideoComposer.compose_video() with {len(processed_images)} images...")
                
                # CRITICAL DEBUG: Log what data we're passing to VideoComposer
                logger.info(f"ðŸŽ¬ COMPOSITION DATA DEBUG:")
                logger.info(f"ðŸŽ¬   - transcription: '{transcription[:100]}...' ({len(transcription)} chars)")
                logger.info(f"ðŸŽ¬   - emphasized_segments count: {len(emphasized_segments)}")
                logger.info(f"ðŸŽ¬   - enriched_entities count: {len(enriched_entities)}")
                logger.info(f"ðŸŽ¬   - processed_images count: {len(processed_images)}")
                logger.info(f"ðŸŽ¬   - content_matches count: {len(content_matches)}")
                
                if emphasized_segments:
                    logger.info(f"ðŸŽ¬   - emphasis sample: {emphasized_segments[0]}")
                if enriched_entities:
                    logger.info(f"ðŸŽ¬   - entity sample: {enriched_entities[0]}")
                if processed_images:
                    logger.info(f"ðŸŽ¬   - image sample: {processed_images[0]}")
                
                # Compose the enhanced video using the sophisticated system
                composition_result = await video_composer.compose_video(
                    composition_data, 
                    str(enhanced_video_path)
                )
                
                logger.info(f"ðŸŽ¬ VideoComposer result: success={composition_result.success}")
                if hasattr(composition_result, 'error_message') and composition_result.error_message:
                    logger.error(f"ðŸŽ¬ VideoComposer error: {composition_result.error_message}")
                
                if composition_result.success:
                    logger.info(f"ðŸŽ¬ VideoComposer enhanced video successfully: {enhanced_video_path}")
                    logger.info(f"ðŸŽ¬ Applied {composition_result.total_overlays_applied} overlays, {composition_result.total_effects_applied} effects")
                    return str(enhanced_video_path)
                else:
                    logger.error(f"ðŸŽ¬ VideoComposer failed: {composition_result.error_message}")
                    logger.info(f"ðŸŽ¬ Falling back to base video file: {enhanced_video_path}")
                    return str(enhanced_video_path)
                    
            except Exception as composer_error:
                import traceback
                logger.error(f"ðŸŽ¬ VideoComposer import/execution failed: {composer_error}")
                logger.error(f"ðŸŽ¬ Full VideoComposer traceback: {traceback.format_exc()}")
                logger.info(f"ðŸŽ¬ Using base video file as fallback: {enhanced_video_path}")
                return str(enhanced_video_path)
            

            
        except Exception as e:
            logger.error(f"Failed to create enhanced video with VideoComposer: {e}")
            import traceback
            logger.error(f"VideoComposer exception details: {traceback.format_exc()}")
            
            # Always fallback to copying original video to ensure we have a real video file
            try:
                import shutil
                enhanced_video_filename = f"enhanced_video_{int(time.time())}.mp4"
                enhanced_video_path = output_path / enhanced_video_filename
                
                if Path(video_path).exists():
                    logger.info(f"Exception fallback: copying original video {video_path} -> {enhanced_video_path}")
                    await asyncio.get_event_loop().run_in_executor(
                        None, shutil.copy2, video_path, enhanced_video_path
                    )
                    return str(enhanced_video_path)
                else:
                    logger.error(f"Exception fallback: original video not found: {video_path}")
                    return video_path
            except Exception as fallback_error:
                logger.error(f"Fallback video creation also failed: {fallback_error}")
                return video_path
    
    def _calculate_quality_metrics(self, result: ProcessingResult) -> ProcessingResult:
        """Calculate overall quality metrics."""
        
        # Overall confidence (weighted average)
        emphasis_weight = 0.3
        entity_weight = 0.3
        image_weight = 0.4
        
        # Emphasis accuracy (based on confidence scores) with type safety
        if result.emphasized_segments:
            confidence_scores = []
            for seg in result.emphasized_segments:
                try:
                    confidence = float(seg['confidence'])
                except (ValueError, TypeError):
                    confidence = 0.0  # Default fallback
                confidence_scores.append(confidence)
            
            result.emphasis_accuracy = sum(confidence_scores) / len(confidence_scores)
        else:
            result.emphasis_accuracy = 0.0
        
        # Entity recognition accuracy (based on image potential) with type safety
        if result.enriched_entities:
            entity_scores = []
            for entity in result.enriched_entities:
                try:
                    if hasattr(entity, 'image_potential'):
                        score = float(entity.image_potential)
                    else:
                        score = 0.7  # Default score
                except (ValueError, TypeError):
                    score = 0.7  # Default fallback
                entity_scores.append(score)
            
            result.entity_recognition_accuracy = sum(entity_scores) / len(entity_scores)
        else:
            result.entity_recognition_accuracy = 0.0
        
        # Image match quality (based on match scores) with type safety
        if result.content_matches:
            match_scores = []
            for match in result.content_matches:
                try:
                    score = float(getattr(match, 'match_score', 0.7))
                except (ValueError, TypeError):
                    score = 0.7  # Default fallback
                match_scores.append(score)
            
            result.image_match_quality = sum(match_scores) / len(match_scores)
        else:
            result.image_match_quality = 0.0
        
        # Overall confidence
        result.overall_confidence = (
            result.emphasis_accuracy * emphasis_weight +
            result.entity_recognition_accuracy * entity_weight +
            result.image_match_quality * image_weight
        )
        
        return result
    
    async def _save_results(self, result: ProcessingResult, output_path: Path):
        """Save processing results to files."""
        
        # Save overlay configuration (JSON format for video editor)
        overlay_config = {
            'video_path': result.video_path,
            'processing_time': result.processing_time,
            'confidence': result.overall_confidence,
            'overlays': []
        }
        
        for match in result.content_matches:
            overlay_config['overlays'].append({
                'image_path': getattr(match, 'processed_path', ''),
                'start_time': getattr(match, 'start_time', 0.0),
                'duration': getattr(match, 'duration', 3.0),
                'position': getattr(match, 'position', 'top-right'),
                'opacity': getattr(match, 'opacity', 0.9),
                'entity_name': getattr(match.image_result, 'entity_name', '') if hasattr(match, 'image_result') else ''
            })
        
        # Save configuration file
        config_filename = f"overlay_config_{int(time.time())}.json"
        config_path = output_path / config_filename
        
        import json
        with open(config_path, 'w') as f:
            json.dump(overlay_config, f, indent=2)
        
        result.overlay_config_path = str(config_path)
        
        logger.info(f"Saved overlay configuration to: {config_path}")
    
    def _update_processing_stats(self, result: ProcessingResult):
        """Update processing statistics."""
        
        # Update stats with type safety
        try:
            current_total = int(self.processing_stats.get('total_videos_processed', 0))
        except (ValueError, TypeError):
            current_total = 0
        self.processing_stats['total_videos_processed'] = current_total + 1
        
        try:
            current_time = float(self.processing_stats.get('total_processing_time', 0.0))
        except (ValueError, TypeError):
            current_time = 0.0
        self.processing_stats['total_processing_time'] = current_time + result.processing_time
        
        # Calculate average with type safety
        try:
            total_videos = int(self.processing_stats.get('total_videos_processed', 1))
            total_time = float(self.processing_stats.get('total_processing_time', 0.0))
        except (ValueError, TypeError):
            total_videos = 1
            total_time = 0.0
        
        self.processing_stats['average_processing_time'] = (
            total_time / max(total_videos, 1)  # Prevent division by zero
        )
        
        # Success rate (based on confidence threshold)
        success_threshold = 0.6
        if result.overall_confidence >= success_threshold:
            current_success_rate = self.processing_stats.get('successful_videos', 0)
            # Ensure type safety - convert to int if it's a string
            try:
                current_success_count = int(current_success_rate) if current_success_rate is not None else 0
            except (ValueError, TypeError):
                current_success_count = 0
            self.processing_stats['successful_videos'] = current_success_count + 1
        
        # Calculate success rate with type safety
        try:
            successful_videos = int(self.processing_stats.get('successful_videos', 0))
        except (ValueError, TypeError):
            successful_videos = 0
        
        self.processing_stats['success_rate'] = (
            successful_videos / total_videos
        )
    
    def get_processing_statistics(self) -> Dict:
        """Get processing pipeline statistics."""
        return {
            **self.processing_stats,
            'config': {
                'video_format': self.config.video_format,
                'max_overlays': self.config.max_overlays_per_video,
                'emphasis_threshold': self.config.emphasis_threshold,
                'entity_threshold': self.config.entity_confidence_threshold
            }
        }
    
    async def process_batch(self, video_paths: List[str], output_dir: str = "./output") -> List[ProcessingResult]:
        """Process multiple videos in parallel."""
        
        logger.info(f"Starting batch processing of {len(video_paths)} videos")
        
        # Process videos in parallel (limit concurrency to avoid resource exhaustion)
        semaphore = asyncio.Semaphore(3)  # Max 3 concurrent processes
        
        async def process_with_semaphore(video_path: str):
            async with semaphore:
                return await self.process_video(video_path, output_dir)
        
        tasks = [process_with_semaphore(path) for path in video_paths]
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # Filter successful results
        successful_results = [
            result for result in results 
            if isinstance(result, ProcessingResult)
        ]
        
        logger.info(f"Batch processing completed: {len(successful_results)}/{len(video_paths)} successful")
        return successful_results 