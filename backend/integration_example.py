"""
Integration Example: Image Curation + Storage System

This example demonstrates how the new Storage System (Days 25-26) integrates 
with the existing Image Curation System (Days 23-24) to create a complete 
image processing pipeline.

Flow:
1. Curate images using ML (CLIP + quality assessment)
2. Store best images with multi-size processing 
3. Serve optimized images via CDN
"""

import asyncio
import logging
from typing import List
from datetime import datetime

# Phase 2 Days 23-24: Image Curation
from app.services.images.curation import ImageCurator, CuratedImage
from app.services.images.curation.models import WordContext
from app.services.audio.entity_enricher import EnrichedEntity

# Phase 2 Days 25-26: Image Storage 
from app.services.images.storage import (
    StorageConfig, 
    ImageStorageManager, 
    StoredImage
)

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class IntegratedImagePipeline:
    """
    Complete image pipeline combining curation and storage.
    
    Features:
    - ML-powered image selection using CLIP and quality assessment
    - Multi-size image processing and optimization
    - CDN distribution for fast delivery
    - Intelligent caching and deduplication
    """
    
    def __init__(self, storage_config: StorageConfig):
        """Initialize the integrated pipeline."""
        self.curator = ImageCurator()
        self.storage_manager = ImageStorageManager(storage_config)
        
        logger.info("Integrated image pipeline initialized")
    
    async def process_entity_images(
        self,
        entity: EnrichedEntity,
        context: WordContext,
        max_images: int = 3
    ) -> List[StoredImage]:
        """
        Complete pipeline: curate â†’ store â†’ serve.
        
        Args:
            entity: The enriched entity (person, location, etc.)
            context: Context around the emphasized word
            max_images: Maximum number of images to store
            
        Returns:
            List of StoredImage objects with CDN URLs
        """
        logger.info(f"Processing images for entity: {entity.name}")
        
        try:
            # 1. CURATION: Use ML to select best images
            logger.info("ğŸ¯ Starting ML-powered image curation...")
            
            curated_images = await self.curator.curate_images_for_entity(
                entity=entity,
                context=context,
                max_results=max_images * 2  # Get extra to choose from
            )
            
            if not curated_images:
                logger.warning(f"No images found for entity: {entity.name}")
                return []
            
            logger.info(f"Found {len(curated_images)} curated images")
            
            # 2. STORAGE: Process and store the best images
            stored_images = []
            
            for i, curated_image in enumerate(curated_images[:max_images]):
                try:
                    logger.info(f"ğŸ“¦ Storing image {i+1}/{max_images}...")
                    
                    # Prepare metadata for storage
                    storage_metadata = {
                        'entity_name': entity.name,
                        'entity_type': entity.entity_type,
                        'relevance_score': curated_image.relevance_score,
                        'quality_score': curated_image.quality_score,
                        'source_provider': curated_image.source.provider,
                        'curated_at': datetime.utcnow().isoformat(),
                        'context_sentence': context.sentence
                    }
                    
                    # Store with multi-size processing
                    stored_image = await self.storage_manager.store_curated_image(
                        image_url=curated_image.source.url,
                        entity_id=f"{entity.entity_type}_{entity.name.lower().replace(' ', '_')}",
                        metadata=storage_metadata
                    )
                    
                    stored_images.append(stored_image)
                    
                    # Log success with CDN URLs
                    logger.info(f"âœ… Stored image with {len(stored_image.get_available_sizes())} sizes")
                    for size in stored_image.get_available_sizes():
                        logger.info(f"   {size}: {stored_image.get_url(size)}")
                    
                except Exception as e:
                    logger.error(f"Failed to store image {i+1}: {e}")
                    continue
            
            logger.info(f"ğŸ‰ Successfully processed {len(stored_images)} images for {entity.name}")
            return stored_images
            
        except Exception as e:
            logger.error(f"Pipeline failed for entity {entity.name}: {e}")
            return []
    
    async def get_entity_image_urls(
        self, 
        entity_name: str, 
        size: str = 'preview'
    ) -> List[str]:
        """
        Get CDN URLs for an entity's stored images.
        
        Args:
            entity_name: Name of the entity
            size: Image size (thumbnail, preview, overlay, full)
            
        Returns:
            List of CDN URLs ready for display
        """
        # TODO: In production, query database for stored images by entity
        # For now, return empty list
        logger.info(f"Would retrieve {size} images for: {entity_name}")
        return []


async def demo_integration():
    """Demonstrate the complete integrated pipeline."""
    print("ğŸ”„ Integrated Image Pipeline Demo")
    print("=" * 60)
    
    # Configuration
    storage_config = StorageConfig(
        processed_bucket="video-enhancement-production",
        cdn_domain="images.videoenhancer.com",
        cloudfront_distribution_id="E123456789ABCD"
    )
    
    # Initialize pipeline
    pipeline = IntegratedImagePipeline(storage_config)
    
    # Sample entities (in production, these come from your NLP pipeline)
    sample_entities = [
        EnrichedEntity(
            name="Elon Musk",
            entity_type="PERSON",
            confidence=0.95,
            description="CEO of Tesla and SpaceX",
            categories=["business", "technology", "space"]
        ),
        EnrichedEntity(
            name="Paris",
            entity_type="LOCATION", 
            confidence=0.92,
            description="Capital city of France",
            categories=["travel", "culture", "europe"]
        )
    ]
    
    # Sample context (from emphasized word detection)
    context = WordContext(
        word="entrepreneur",
        sentence="He is a successful entrepreneur who changed the world.",
        emphasis_strength=0.85,
        position=3
    )
    
    print("ğŸ¯ Processing Sample Entities:")
    print(f"   â€¢ Storage Bucket: {storage_config.processed_bucket}")
    print(f"   â€¢ CDN Domain: {storage_config.cdn_domain}")
    print(f"   â€¢ Image Sizes: {list(storage_config.image_sizes.keys())}")
    print()
    
    # Process each entity
    for entity in sample_entities:
        print(f"ğŸ” Processing: {entity.name} ({entity.entity_type})")
        
        # Note: In actual usage with AWS credentials, this would work
        print(f"   Would curate images using CLIP similarity")
        print(f"   Would assess quality using computer vision")
        print(f"   Would store optimized images in S3")
        print(f"   Would generate CDN URLs:")
        
        # Show what the URLs would look like
        entity_id = f"{entity.entity_type.lower()}_{entity.name.lower().replace(' ', '_')}"
        for size in storage_config.image_sizes.keys():
            mock_url = f"https://{storage_config.cdn_domain}/entities/{entity_id}/[hash]/{size}.webp"
            print(f"     {size}: {mock_url}")
        
        print()
    
    print("âœ¨ Integration Features Demonstrated:")
    print("   ğŸ§  ML-powered image curation (CLIP + quality assessment)")
    print("   ğŸ–¼ï¸  Multi-size image processing (WebP optimized)")
    print("   â˜ï¸  AWS S3 storage with metadata")
    print("   ğŸš€ CloudFront CDN for fast delivery")
    print("   ğŸ”„ Content deduplication via hashing")
    print("   ğŸ“Š Quality and relevance scoring")
    print("   ğŸ›¡ï¸  Legal compliance validation")
    print("   ğŸ’¾ Redis caching for performance")


async def demo_video_processing_workflow():
    """Show how this fits into the complete video processing workflow."""
    print("\nğŸ¬ Complete Video Processing Workflow")
    print("=" * 60)
    
    print("ğŸ“ Phase 1 (COMPLETED):")
    print("   âœ… Audio extraction from video")
    print("   âœ… Speech-to-text transcription")
    print("   âœ… Emphasis detection using prosodic features")
    print("   âœ… Named entity recognition")
    print("   âœ… Entity enrichment with descriptions")
    print()
    
    print("ğŸ“ Phase 2 Week 5 (COMPLETED):")
    print("   âœ… Multi-source image retrieval (Unsplash, Pexels, Wikimedia)")
    print("   âœ… Provider abstraction and unified interface")
    print()
    
    print("ğŸ“ Phase 2 Days 23-24 (COMPLETED):")
    print("   âœ… CLIP model for image-text similarity")
    print("   âœ… Computer vision quality assessment")
    print("   âœ… Face detection for person entities")
    print("   âœ… Legal compliance validation")
    print("   âœ… Redis-backed caching system")
    print()
    
    print("ğŸ“ Phase 2 Days 25-26 (COMPLETED TODAY!):")
    print("   âœ… S3 storage with multi-size processing")
    print("   âœ… CloudFront CDN distribution")
    print("   âœ… Smart cropping and WebP optimization")
    print("   âœ… Content deduplication")
    print("   âœ… Cache invalidation management")
    print()
    
    print("ğŸ”„ Complete Workflow:")
    print("   1. ğŸµ Extract audio from TikTok/Reels video")
    print("   2. ğŸ“ Transcribe speech with timestamps")
    print("   3. ğŸ’¥ Detect emphasized words using ML")
    print("   4. ğŸ·ï¸  Extract and enrich entities")
    print("   5. ğŸ” Search multiple image providers")
    print("   6. ğŸ§  Use CLIP to score image relevance")
    print("   7. ğŸ–¼ï¸  Process images into multiple sizes")
    print("   8. â˜ï¸  Store in S3 with CDN distribution")
    print("   9. ğŸ¬ Overlay images on video at emphasis points")
    print("   10. ğŸ“± Export enhanced video for social media")


if __name__ == "__main__":
    print("ğŸ”„ Integration Pipeline Demo")
    
    # Run demonstrations
    asyncio.run(demo_integration())
    asyncio.run(demo_video_processing_workflow()) 